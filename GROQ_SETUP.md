# Groq (Llama) Setup Complete âœ…

## What Was Changed

1. âœ… **Code Updated**: `src/config.py` now supports Groq provider
2. âœ… **Dependencies**: Added `langchain-groq>=0.1.0` to `pyproject.toml`
3. âœ… **.env File**: Updated to use Groq provider and Llama models
4. âœ… **Model Configuration**: Set to `llama-3.1-70b-versatile` (high quality)

## Installation Required

**You need to install the Groq package:**

```bash
pip install langchain-groq
# or
uv sync
```

## .env File Configuration

Your `.env` file should have:

```bash
# Groq API Key (get from https://console.groq.com)
GROQ_API_KEY=your_groq_api_key_here

# Model Configuration
LAYER1_MODEL=llama-3.1-70b-versatile
LAYER1_PROVIDER=groq
LAYER2_MODEL=llama-3.1-70b-versatile
LAYER2_PROVIDER=groq
LAYER3_MODEL=llama-3.1-70b-versatile
LAYER3_PROVIDER=groq

# Visual (Gemini)
VISUAL_MODEL=gemini-1.5-pro
VISUAL_PROVIDER=google
GOOGLE_API_KEY=your_google_api_key_here
```

## Testing

After installing `langchain-groq` and adding your `GROQ_API_KEY`:

```bash
# Test configuration
python -c "from src.config import get_model_metadata; import json; print(json.dumps(get_model_metadata(), indent=2))"

# Test interim audit
python -m src.graph "https://github.com/sumeyaaaa/Automaton-Auditor" "reports/interim_report.pdf" --interim

# Test full audit
python -m src.graph "https://github.com/sumeyaaaa/Automaton-Auditor" "reports/interim_report.pdf"
```

## Available Groq Models

- `llama-3.1-70b-versatile` - Best quality (default)
- `llama-3.1-8b-instant` - Faster, cheaper
- `llama-3.3-70b-versatile` - Latest version
- `mixtral-8x7b-32768` - Alternative option

## Get Your API Key

1. Go to https://console.groq.com
2. Sign up / Log in
3. Create an API key
4. Add it to your `.env` file as `GROQ_API_KEY`

## Benefits of Groq

- âš¡ **Very Fast** - Optimized inference speed
- ðŸ’° **Free Tier** - Generous free limits
- ðŸ”“ **Open Source** - Llama models are transparent
- ðŸŽ¯ **Great for Code** - Excellent at technical analysis

