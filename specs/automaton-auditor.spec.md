# Automaton Auditor - Product Specification

## Overview

The Automaton Auditor is an autonomous quality assurance system that evaluates code repositories and documentation against a machine-readable rubric. It operates as a "Digital Courtroom" where specialized AI agents collect evidence, deliberate through distinct judicial personas, and synthesize final verdicts with actionable remediation plans.

## Problem Statement

In AI-native enterprises, the volume of code generated by autonomous agents will outpace human review capacity by orders of magnitude. If 1,000 agents generate features concurrently, humans cannot manually review every pull request. The bottleneck shifts from generating code to evaluating it.

**Core Challenge:** How do we maintain quality at scale when human review is no longer feasible?

## Solution

An automated auditing swarm that:
- **Forensically analyzes** code repositories to verify structure, security, and architecture
- **Objectively evaluates** documentation for accuracy and theoretical depth
- **Applies nuanced judgment** through multiple judicial perspectives (critical, optimistic, pragmatic)
- **Synthesizes verdicts** using deterministic conflict resolution rules
- **Provides actionable feedback** with specific file-level remediation instructions

## Core Capabilities

### 1. Forensic Evidence Collection
The system collects objective facts about:
- **Code Structure**: Verifies existence of typed state management, parallel graph orchestration, security sandboxing
- **Git History**: Analyzes commit progression patterns to assess development methodology
- **Documentation Accuracy**: Cross-references claims in PDF reports against actual repository files
- **Architectural Diagrams**: Validates that visual representations match implementation

### 2. Multi-Perspective Evaluation
Three distinct judicial personas evaluate the same evidence:
- **Prosecutor**: Critical lens focusing on gaps, security flaws, and missing requirements
- **Defense**: Optimistic lens rewarding effort, intent, and creative solutions
- **Tech Lead**: Pragmatic lens evaluating functionality, maintainability, and technical debt

### 3. Deterministic Synthesis
Final scores are determined by hardcoded rules, not averaging:
- Security vulnerabilities cap scores regardless of other factors
- Factual evidence overrules judicial opinions
- High disagreement triggers re-evaluation protocols
- All conflicts are documented with dissent summaries

### 4. Actionable Reporting
Outputs include:
- Executive summary with overall score
- Per-criterion breakdown with all three judge opinions
- Dissent summaries explaining conflicts
- Prioritized remediation plans with specific file-level instructions

## Inputs

1. **GitHub Repository URL**: The codebase to be audited
2. **PDF Report Path**: Documentation describing the implementation
3. **Rubric Configuration**: Machine-readable JSON defining evaluation criteria (10 dimensions)

## Outputs

1. **Structured Markdown Report** containing:
   - Audit metadata (repository, commit hash, models used)
   - Executive summary with overall score (1-5 scale)
   - Per-criterion analysis with:
     - Final synthesized score
     - All three judge opinions with arguments
     - Dissent summaries (when applicable)
     - Specific remediation instructions
   - Comprehensive remediation plan prioritized by severity

## Key Principles

### Separation of Concerns
- **Detectives** collect facts only (no opinions, no scores)
- **Judges** interpret evidence and assign scores
- **Chief Justice** synthesizes using deterministic rules

### Evidence-Based Evaluation
- All claims must be backed by forensic evidence
- Hallucinations (claims about non-existent files) are flagged
- Confidence scores justify evidence quality

### Reproducibility
- Deterministic synthesis rules ensure consistent scoring
- Git commit hashes and model metadata included in reports
- All evidence locations are cited and traceable

### Scalability
- Parallel execution of detectives and judges
- Sandboxed repository cloning for security
- Observable via LangSmith tracing

## Success Criteria

The system successfully audits a repository when it:
1. ✅ Collects evidence for all 10 rubric dimensions
2. ✅ Produces distinct opinions from all three judges
3. ✅ Synthesizes scores using deterministic rules
4. ✅ Generates a complete report with actionable remediation
5. ✅ Identifies security vulnerabilities and architectural gaps
6. ✅ Cross-references documentation claims against codebase reality

## Non-Goals

- Real-time streaming UI
- Persistent database storage
- Human-in-the-loop during audit execution
- Multi-language repository analysis (Python only)
- Automated code fixes (evaluation only)

## Use Cases

1. **Peer Review Automation**: Audit peer submissions in educational programs
2. **Security Audits**: Automated vulnerability detection in PR pipelines
3. **Compliance Governance**: Ensure ISO/SOC2 compliance in real-time
4. **Architectural Review**: Prevent "spaghetti code" before merge
5. **Self-Assessment**: Teams auditing their own codebases

## Future Enhancements

- Multi-round debate cycles for high-variance criteria
- Support for additional programming languages
- Integration with CI/CD pipelines
- Custom rubric creation interface
- Historical trend analysis across audits




